{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7911964,"sourceType":"datasetVersion","datasetId":4648219}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Information\n\nAuthor: Thanh Liem (liemkg1234@gmail.com)\n\nTask: Sentiment Analysis\n\nSpoce: Text Classification\n\nModel: DistilBERT (https://huggingface.co/distilbert/distilbert-base-uncased)\n\nDataset: imdb (https://huggingface.co/datasets/imdb)\n\nMetric: Accuracy, Precision, Recall, F1\n\nResult: Weight and ONNX Quantized (https://github.com/huggingface/optimum)\n\nHours Used: 8","metadata":{}},{"cell_type":"markdown","source":"# Package","metadata":{}},{"cell_type":"code","source":"!pip install transformers datasets evaluate accelerate \n!pip uninstall -y wandb \n!pip install --quiet optimum[exporters,onnxruntime-gpu]","metadata":{"execution":{"iopub.status.busy":"2024-03-22T06:23:02.869636Z","iopub.execute_input":"2024-03-22T06:23:02.870601Z","iopub.status.idle":"2024-03-22T06:23:29.820060Z","shell.execute_reply.started":"2024-03-22T06:23:02.870546Z","shell.execute_reply":"2024-03-22T06:23:29.819046Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.38.2)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\nRequirement already satisfied: evaluate in /opt/conda/lib/python3.10/site-packages (0.4.1)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.28.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.21.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (11.0.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.1.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->datasets) (2024.3.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[33mWARNING: Skipping wandb as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Hyper Parameters","metadata":{}},{"cell_type":"code","source":"model_name = \"distilbert/distilbert-base-uncased\"\nfolder_save = \"/kaggle/working/distilbert_imdb\"\nepochs = 10\nbatch_size = 16\nmax_input_length = 512","metadata":{"execution":{"iopub.status.busy":"2024-03-22T06:23:29.822626Z","iopub.execute_input":"2024-03-22T06:23:29.823431Z","iopub.status.idle":"2024-03-22T06:23:29.828297Z","shell.execute_reply.started":"2024-03-22T06:23:29.823392Z","shell.execute_reply":"2024-03-22T06:23:29.827383Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Load dataset\n\nDataset Summary\n\nLarge Movie Review Dataset. This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well.","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset, DatasetDict\n\nraw_datasets = load_dataset(\"imdb\")\n\nraw_datasets = DatasetDict({\n    'train': raw_datasets['train'].select(range(5000, raw_datasets['train'].num_rows)),\n    'validation': raw_datasets['train'].select(range(5000)),\n    'test': raw_datasets['test']\n})\n\nprint(raw_datasets)\nprint(raw_datasets['test'][0])","metadata":{"execution":{"iopub.status.busy":"2024-03-22T06:23:29.829640Z","iopub.execute_input":"2024-03-22T06:23:29.830265Z","iopub.status.idle":"2024-03-22T06:23:30.322966Z","shell.execute_reply.started":"2024-03-22T06:23:29.830234Z","shell.execute_reply":"2024-03-22T06:23:30.321972Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"962a0273e40741bc98632ea6329d2bd8"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 20000\n    })\n    validation: Dataset({\n        features: ['text', 'label'],\n        num_rows: 5000\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 25000\n    })\n})\n{'text': 'I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn\\'t match the background, and painfully one-dimensional characters cannot be overcome with a \\'sci-fi\\' setting. (I\\'m sure there are those of you out there who think Babylon 5 is good sci-fi TV. It\\'s not. It\\'s clichéd and uninspiring.) While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf. Star Trek). It may treat important issues, yet not as a serious philosophy. It\\'s really difficult to care about the characters here as they are not simply foolish, just missing a spark of life. Their actions and reactions are wooden and predictable, often painful to watch. The makers of Earth KNOW it\\'s rubbish as they have to always say \"Gene Roddenberry\\'s Earth...\" otherwise people would not continue watching. Roddenberry\\'s ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space. Spoiler. So, kill off a main character. And then bring him back as another actor. Jeeez! Dallas all over again.', 'label': 0}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Preprocess","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\nfrom transformers import DataCollatorWithPadding\n\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\ndef preprocess_function(examples):\n    return tokenizer(examples[\"text\"], max_length=max_input_length, truncation=True, padding='max_length')\n\ntokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\nprint(tokenized_datasets)\nprint(tokenized_datasets['test'][0])","metadata":{"execution":{"iopub.status.busy":"2024-03-22T06:23:30.325342Z","iopub.execute_input":"2024-03-22T06:23:30.325669Z","iopub.status.idle":"2024-03-22T06:24:01.070236Z","shell.execute_reply.started":"2024-03-22T06:23:30.325642Z","shell.execute_reply":"2024-03-22T06:24:01.069261Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/20 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7bea4d0afb64e13ae4c0096256a3375"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d40ab5a668f467eaed2070dfe5a8318"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/25 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa5190572e4b4b9dbb6aa1ea8349931e"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 20000\n    })\n    validation: Dataset({\n        features: ['text', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 5000\n    })\n    test: Dataset({\n        features: ['text', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 25000\n    })\n})\n{'text': 'I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn\\'t match the background, and painfully one-dimensional characters cannot be overcome with a \\'sci-fi\\' setting. (I\\'m sure there are those of you out there who think Babylon 5 is good sci-fi TV. It\\'s not. It\\'s clichéd and uninspiring.) While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf. Star Trek). It may treat important issues, yet not as a serious philosophy. It\\'s really difficult to care about the characters here as they are not simply foolish, just missing a spark of life. Their actions and reactions are wooden and predictable, often painful to watch. The makers of Earth KNOW it\\'s rubbish as they have to always say \"Gene Roddenberry\\'s Earth...\" otherwise people would not continue watching. Roddenberry\\'s ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space. Spoiler. So, kill off a main character. And then bring him back as another actor. Jeeez! Dallas all over again.', 'label': 0, 'input_ids': [101, 1045, 2293, 16596, 1011, 10882, 1998, 2572, 5627, 2000, 2404, 2039, 2007, 1037, 2843, 1012, 16596, 1011, 10882, 5691, 1013, 2694, 2024, 2788, 2104, 11263, 25848, 1010, 2104, 1011, 12315, 1998, 28947, 1012, 1045, 2699, 2000, 2066, 2023, 1010, 1045, 2428, 2106, 1010, 2021, 2009, 2003, 2000, 2204, 2694, 16596, 1011, 10882, 2004, 17690, 1019, 2003, 2000, 2732, 10313, 1006, 1996, 2434, 1007, 1012, 10021, 4013, 3367, 20086, 2015, 1010, 10036, 19747, 4520, 1010, 25931, 3064, 22580, 1010, 1039, 2290, 2008, 2987, 1005, 1056, 2674, 1996, 4281, 1010, 1998, 16267, 2028, 1011, 8789, 3494, 3685, 2022, 9462, 2007, 1037, 1005, 16596, 1011, 10882, 1005, 4292, 1012, 1006, 1045, 1005, 1049, 2469, 2045, 2024, 2216, 1997, 2017, 2041, 2045, 2040, 2228, 17690, 1019, 2003, 2204, 16596, 1011, 10882, 2694, 1012, 2009, 1005, 1055, 2025, 1012, 2009, 1005, 1055, 18856, 17322, 2094, 1998, 4895, 7076, 8197, 4892, 1012, 1007, 2096, 2149, 7193, 2453, 2066, 7603, 1998, 2839, 2458, 1010, 16596, 1011, 10882, 2003, 1037, 6907, 2008, 2515, 2025, 2202, 2993, 5667, 1006, 12935, 1012, 2732, 10313, 1007, 1012, 2009, 2089, 7438, 2590, 3314, 1010, 2664, 2025, 2004, 1037, 3809, 4695, 1012, 2009, 1005, 1055, 2428, 3697, 2000, 2729, 2055, 1996, 3494, 2182, 2004, 2027, 2024, 2025, 3432, 13219, 1010, 2074, 4394, 1037, 12125, 1997, 2166, 1012, 2037, 4506, 1998, 9597, 2024, 4799, 1998, 21425, 1010, 2411, 9145, 2000, 3422, 1012, 1996, 11153, 1997, 3011, 2113, 2009, 1005, 1055, 29132, 2004, 2027, 2031, 2000, 2467, 2360, 1000, 4962, 8473, 4181, 9766, 1005, 1055, 3011, 1012, 1012, 1012, 1000, 4728, 2111, 2052, 2025, 3613, 3666, 1012, 8473, 4181, 9766, 1005, 1055, 11289, 2442, 2022, 3810, 1999, 2037, 8753, 2004, 2023, 10634, 1010, 10036, 1010, 9996, 5493, 1006, 3666, 2009, 2302, 4748, 16874, 7807, 2428, 7545, 2023, 2188, 1007, 19817, 6784, 4726, 19817, 19736, 3372, 1997, 1037, 2265, 13891, 2015, 2046, 2686, 1012, 27594, 2121, 1012, 2061, 1010, 3102, 2125, 1037, 2364, 2839, 1012, 1998, 2059, 3288, 2032, 2067, 2004, 2178, 3364, 1012, 15333, 4402, 2480, 999, 5759, 2035, 2058, 2153, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Metrics","metadata":{}},{"cell_type":"code","source":"import evaluate\nimport numpy as np\n\naccuracy = evaluate.load(\"accuracy\")\nprecision = evaluate.load(\"precision\")\nrecall = evaluate.load(\"recall\")\nf1 = evaluate.load(\"f1\")\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n    \n    accuracy_result = accuracy.compute(predictions=predictions, references=labels)\n    precision_result = precision.compute(predictions=predictions, references=labels, average='macro')\n    recall_result = recall.compute(predictions=predictions, references=labels, average='macro')\n    f1_result = f1.compute(predictions=predictions, references=labels, average='macro')\n\n    return {\n        'accuracy': accuracy_result[\"accuracy\"],\n        'precision': precision_result[\"precision\"],\n        'recall': recall_result[\"recall\"],\n        'f1': f1_result[\"f1\"],\n    }","metadata":{"execution":{"iopub.status.busy":"2024-03-22T06:24:01.071275Z","iopub.execute_input":"2024-03-22T06:24:01.071542Z","iopub.status.idle":"2024-03-22T06:24:02.628820Z","shell.execute_reply.started":"2024-03-22T06:24:01.071504Z","shell.execute_reply":"2024-03-22T06:24:02.628050Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\nlabel2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}\n\nfrom transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name, num_labels=2, id2label=id2label, label2id=label2id\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-22T06:24:02.629825Z","iopub.execute_input":"2024-03-22T06:24:02.630093Z","iopub.status.idle":"2024-03-22T06:24:02.854848Z","shell.execute_reply.started":"2024-03-22T06:24:02.630068Z","shell.execute_reply":"2024-03-22T06:24:02.853954Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Fine-tune","metadata":{}},{"cell_type":"code","source":"args = TrainingArguments(\n    output_dir=folder_save,\n    learning_rate=2e-5,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    num_train_epochs=epochs,\n    weight_decay=0.01,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    push_to_hub=False,\n    report_to=None,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-03-22T06:24:02.856140Z","iopub.execute_input":"2024-03-22T06:24:02.856528Z","iopub.status.idle":"2024-03-22T07:48:10.851224Z","shell.execute_reply.started":"2024-03-22T06:24:02.856492Z","shell.execute_reply":"2024-03-22T07:48:10.849358Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5602' max='12500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 5602/12500 1:24:04 < 1:43:33, 1.11 it/s, Epoch 8.96/20]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.277700</td>\n      <td>0.315077</td>\n      <td>0.879800</td>\n      <td>0.500000</td>\n      <td>0.439900</td>\n      <td>0.468029</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.170800</td>\n      <td>0.282552</td>\n      <td>0.905400</td>\n      <td>0.500000</td>\n      <td>0.452700</td>\n      <td>0.475176</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.118900</td>\n      <td>0.267050</td>\n      <td>0.931400</td>\n      <td>0.500000</td>\n      <td>0.465700</td>\n      <td>0.482241</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.059400</td>\n      <td>0.586662</td>\n      <td>0.841200</td>\n      <td>0.500000</td>\n      <td>0.420600</td>\n      <td>0.456876</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.031600</td>\n      <td>0.743903</td>\n      <td>0.857400</td>\n      <td>0.500000</td>\n      <td>0.428700</td>\n      <td>0.461613</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.023500</td>\n      <td>0.779007</td>\n      <td>0.861800</td>\n      <td>0.500000</td>\n      <td>0.430900</td>\n      <td>0.462885</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.016700</td>\n      <td>0.615299</td>\n      <td>0.901000</td>\n      <td>0.500000</td>\n      <td>0.450500</td>\n      <td>0.473961</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.010400</td>\n      <td>1.112934</td>\n      <td>0.855200</td>\n      <td>0.500000</td>\n      <td>0.427600</td>\n      <td>0.460975</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[14], line 24\u001b[0m\n\u001b[1;32m      1\u001b[0m args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m      2\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39mfolder_save,\n\u001b[1;32m      3\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2e-5\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     report_to\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     14\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     15\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     16\u001b[0m     args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[1;32m     22\u001b[0m )\n\u001b[0;32m---> 24\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1624\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1622\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1629\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1966\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1960\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m   1961\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1964\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1965\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m-> 1966\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1967\u001b[0m ):\n\u001b[1;32m   1968\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1969\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   1970\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"Do validation dataset chỉ có 1 class nên metrics không tốt => cần shuffle trước khi split ra cho validation\n\nTuy nhiên trên test dataset vẫn tốt: > 90% ở các chỉ số","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working/distilbert_imdb\n!ls\n!find . -mindepth 1 | grep -v '^./checkpoint-1875' | xargs rm -rf\n!ls","metadata":{"execution":{"iopub.status.busy":"2024-03-22T07:48:10.852081Z","iopub.status.idle":"2024-03-22T07:48:10.852407Z","shell.execute_reply.started":"2024-03-22T07:48:10.852244Z","shell.execute_reply":"2024-03-22T07:48:10.852257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_checkpoint = \"/kaggle/input/distilbert-imdb-finetuned/checkpoint-1875\"\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\n\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\npipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n\nprint(test_dataset[0]['label'])\npipe(test_dataset[0]['text'])","metadata":{"execution":{"iopub.status.busy":"2024-03-22T07:51:11.011273Z","iopub.execute_input":"2024-03-22T07:51:11.012130Z","iopub.status.idle":"2024-03-22T07:51:11.350628Z","shell.execute_reply.started":"2024-03-22T07:51:11.012097Z","shell.execute_reply":"2024-03-22T07:51:11.349711Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"0\n","output_type":"stream"},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"[{'label': 'NEGATIVE', 'score': 0.9980358481407166}]"},"metadata":{}}]},{"cell_type":"markdown","source":"# Eval","metadata":{}},{"cell_type":"code","source":"test_dataset = raw_datasets['test']\nlabel2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}\n\nimport torch\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)\n\n# Original\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\nmodel_checkpoint = \"/kaggle/input/distilbert-imdb-finetuned/checkpoint-1875\"\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\nmodel = model.to(device)\n\n# # ONNX + Quantized\n# from optimum.onnxruntime import ORTModelForSequenceClassification\n# from transformers import AutoTokenizer\n# model_checkpoint = \"/kaggle/input/distilbert-imdb-finetuned/distilbert_imdb_onnx_quantized\"\n# tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n# model = ORTModelForSequenceClassification.from_pretrained(model_checkpoint, use_cache=False)\n\n\npipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, max_length=max_input_length)\n\ntexts = []\nlabels = []\npredictions = []\nfor i in range(len(test_dataset['text'])):\n    prediction_label = pipe(test_dataset[i]['text'])\n    prediction = label2id[prediction_label[0]['label']]\n    # append\n    texts.append(test_dataset[i]['text'])\n    labels.append(test_dataset[i]['label'])\n    predictions.append(prediction)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-22T08:22:41.877643Z","iopub.execute_input":"2024-03-22T08:22:41.877985Z","iopub.status.idle":"2024-03-22T09:23:54.989480Z","shell.execute_reply.started":"2024-03-22T08:22:41.877956Z","shell.execute_reply":"2024-03-22T09:23:54.988026Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"},{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[30], line 45\u001b[0m\n\u001b[1;32m     38\u001b[0m data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m: texts,\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m: labels,\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredict\u001b[39m\u001b[38;5;124m'\u001b[39m: predictions,\n\u001b[1;32m     42\u001b[0m }\n\u001b[1;32m     44\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data)\n\u001b[0;32m---> 45\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mdistilbert_imdb\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     46\u001b[0m df\u001b[38;5;241m.\u001b[39mto_excel(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdistilbert_imdb\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mevaluate\u001b[39;00m\n","\u001b[0;31mNameError\u001b[0m: name 'distilbert_imdb' is not defined"],"ename":"NameError","evalue":"name 'distilbert_imdb' is not defined","output_type":"error"}]},{"cell_type":"code","source":"import pandas as pd\ndata = {\n    'text': texts,\n    'label': labels,\n    'predict': predictions,\n}\n\ndf = pd.DataFrame(data)\ndf.to_csv(f'eval_distilbert_imdb.csv', index=False)\ndf.to_excel(f'eval_distilbert_imdb.xlsx', index=False)\n\nimport evaluate\n\n# Metric\naccuracy_test = evaluate.load(\"accuracy\").compute(predictions=predictions, references=labels)\nprecision_test = evaluate.load(\"precision\").compute(predictions=predictions, references=labels, average='macro')\nrecall_test = evaluate.load(\"recall\").compute(predictions=predictions, references=labels, average='macro')\nf1_test = evaluate.load(\"f1\").compute(predictions=predictions, references=labels, average='macro')\n\nprint(f\"accuracy: {accuracy_test}, precision: {precision_test}, recall: {recall_test}, f1: {f1_test}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-22T09:25:16.336574Z","iopub.execute_input":"2024-03-22T09:25:16.336922Z","iopub.status.idle":"2024-03-22T09:25:25.840903Z","shell.execute_reply.started":"2024-03-22T09:25:16.336897Z","shell.execute_reply":"2024-03-22T09:25:25.839902Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"accuracy: {'accuracy': 0.92656}, precision: {'precision': 0.9265711823076015}, recall: {'recall': 0.92656}, f1: {'f1': 0.9265595187004618}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Interface","metadata":{}},{"cell_type":"code","source":"# Original\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\nmodel_checkpoint = \"/kaggle/input/distilbert-imdb-finetuned/checkpoint-1875\"\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n\n# # ONNX + Quantized\n# from optimum.onnxruntime import ORTModelForSequenceClassification\n# from transformers import AutoTokenizer\n# model_checkpoint = \"/kaggle/input/distilbert-imdb-finetuned/distilbert_imdb_onnx_quantized\"\n# tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n# model = ORTModelForSequenceClassification.from_pretrained(model_checkpoint, use_cache=False)\n\n\npipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, max_length=max_input_length)\npipe(\"Hello\")","metadata":{"execution":{"iopub.status.busy":"2024-03-22T07:48:10.856110Z","iopub.status.idle":"2024-03-22T07:48:10.856561Z","shell.execute_reply.started":"2024-03-22T07:48:10.856314Z","shell.execute_reply":"2024-03-22T07:48:10.856332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ONNX + Quantized","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working/distilbert_imdb\n\n# Export\n!optimum-cli export onnx \\\n  --task text-classification \\\n  -m checkpoint-1875 \\\n  --optimize O1 \\\n  distilbert_imdb_onnx","metadata":{"execution":{"iopub.status.busy":"2024-03-22T07:48:10.858331Z","iopub.status.idle":"2024-03-22T07:48:10.858865Z","shell.execute_reply.started":"2024-03-22T07:48:10.858629Z","shell.execute_reply":"2024-03-22T07:48:10.858648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Quantize\n!optimum-cli onnxruntime quantize \\\n  --avx512 \\\n  --onnx_model distilbert_imdb_onnx \\\n  --output distilbert_imdb_onnx_quantized","metadata":{"execution":{"iopub.status.busy":"2024-03-22T07:48:10.860608Z","iopub.status.idle":"2024-03-22T07:48:10.861192Z","shell.execute_reply.started":"2024-03-22T07:48:10.860955Z","shell.execute_reply":"2024-03-22T07:48:10.860975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r /kaggle/working/kaggle.zip /kaggle/working","metadata":{"execution":{"iopub.status.busy":"2024-03-22T07:48:10.862454Z","iopub.status.idle":"2024-03-22T07:48:10.862947Z","shell.execute_reply.started":"2024-03-22T07:48:10.862687Z","shell.execute_reply":"2024-03-22T07:48:10.862706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from optimum.onnxruntime import ORTModelForSequenceClassification\nfrom transformers import AutoTokenizer\n\nmodel_interface = \"models/distilbert_imdb/distilbert_imdb_onnx_quantized\"\ntokenizer = AutoTokenizer.from_pretrained(model_interface)\nmodel = ORTModelForSequenceClassification.from_pretrained(model_interface, use_cache=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-22T07:48:10.864439Z","iopub.status.idle":"2024-03-22T07:48:10.864896Z","shell.execute_reply.started":"2024-03-22T07:48:10.864676Z","shell.execute_reply":"2024-03-22T07:48:10.864694Z"},"trusted":true},"execution_count":null,"outputs":[]}]}